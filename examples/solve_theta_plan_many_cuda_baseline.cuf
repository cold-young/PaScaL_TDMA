!======================================================================================================================
!> @file        solve_theta_plan_many.f90
!> @brief       This file contains a solver subroutine for the example problem of PaScaL_TDMA.
!> @details     The target example problem is the three-dimensional time-dependent heat conduction problem 
!>              in a unit cube domain applied with the boundary conditions of vertically constant temperature 
!>              and horizontally periodic boundaries.
!> @author      
!>              - Kiha Kim (k-kiha@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>              - Ji-Hoon Kang (jhkang@kisti.re.kr), Korea Institute of Science and Technology Information
!>              - Jung-Il Choi (jic@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>
!> @date        June 2019
!> @version     1.0
!> @par         Copyright
!>              Copyright (c) 2019 Kiha Kim and Jung-Il choi, Yonsei University and 
!>              Ji-Hoon Kang, Korea Institute of Science and Technology Information, All rights reserved.
!> @par         License     
!>              This project is release under the terms of the MIT License (see LICENSE in )
!======================================================================================================================
module solve_theta_cuda

    integer, parameter, private :: BLOCK_DIM_X=8, BLOCK_DIM_Y=8, BLOCK_DIM_Z=8

contains
    !>
    !>
    !> @brief       An example solver for many tridiagonal systems of equations using PaScaL_TDMA.
    !> @details     This subroutine is for many tridiagonal systems of equations.
    !>              It solves the the three-dimensional time-dependent heat conduction problem using PaScaL_TDMA.
    !>              PaScaL_TDMA plans are created for many tridiagonal systems of equations and
    !>              the many tridiagonal systems are solved plane-by-plane.
    !> @param       theta       Main 3-D variable to be solved
    !>
    subroutine solve_theta_plan_many_cuda(theta)

        use omp_lib
        use mpi
        use global
        use mpi_subdomain
        use mpi_topology
        use PaScaL_TDMA_cuda
        use cudafor

        implicit none

        double precision, dimension(0:nx_sub, 0:ny_sub, 0:nz_sub), intent(inout) :: theta
        
        integer :: myrank, ierr
        integer :: time_step        ! Current time step
        double precision :: t_curr  ! Current simulation time
    
        ! Loop and index variables
        integer :: i,j,k
        integer :: ip, jp, kp
        integer :: im, jm, km
        integer :: jem, jep

        integer :: n_thds
        integer :: ti
        integer :: icur, jcur, kcur, n_team, n_remain

        ! Temporary variables for coefficient computations
        double precision :: dedx1, dedx2, dedy3, dedy4, dedz5, dedz6    ! Derivative terms
        double precision :: viscous_e1, viscous_e2, viscous_e3, viscous ! Viscous terms
        double precision :: ebc_down, ebc_up, ebc                       ! Boundary terms
        double precision :: eAPI, eAMI, eACI                            ! Diffusion treatment terms in x-direction
        double precision :: eAPJ, eAMJ, eACJ                            ! Diffusion treatment terms in y-direction
        double precision :: eAPK, eAMK, eACK                            ! Diffusion treatment terms in z-direction
        double precision :: eRHS                                        ! From eAPI to eACK
        double precision :: Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz

        double precision, allocatable, dimension(:, :, :) :: rhs            ! r.h.s. array

        double precision, allocatable, device, dimension(:, :, :)   :: theta_d, rhs_d            ! r.h.s. array
        double precision, allocatable, device, dimension(:,:,:)     :: ap_d, am_d, ac_d, ad_d ! Coefficient of tridiagonal matrix

        type(ptdma_plan_many_cuda) :: px_many, pz_many , py_many  ! Plan for many tridiagonal systems of equations
        double precision, device :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, device :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)

        integer, parameter :: thread_in_x = BLOCK_DIM_X, thread_in_y = BLOCK_DIM_Y, thread_in_z = BLOCK_DIM_Z
        integer :: block_in_x, block_in_y, block_in_z

        type(dim3) :: blocks, threads

        call MPI_Comm_rank( MPI_COMM_WORLD, myrank, ierr)
    
        Ct_half_over_dx = 0.5d0*Ct/dx
        Ct_half_over_dy = 0.5d0*Ct/dy
        Ct_half_over_dz = 0.5d0*Ct/dz

        dmx_sub_d = dmx_sub
        dmy_sub_d = dmy_sub
        dmz_sub_d = dmz_sub
        jpbc_index_d = jpbc_index
        jmbc_index_d = jmbc_index
        thetaBC3_sub_d = thetaBC3_sub
        thetaBC4_sub_d = thetaBC4_sub

        ! Calculating r.h.s.
        allocate( rhs_d(nx_sub-1, ny_sub-1, nz_sub-1))
        allocate( theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub))

        ! Setting the thread and block
        block_in_x = (nx_sub-1)/thread_in_x
        if(block_in_x.eq.0 .or. mod((nx_sub-1), thread_in_x)) then
            print '(a,i5,a,i5)', '[Error] ny_sub-1 should be a multiple of thread_in_x. thread_in_x = ',thread_in_x,', nx_sub-1 = ',nx_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_y = (ny_sub-1)/thread_in_y
        if(block_in_y.eq.0 .or. mod((ny_sub-1), thread_in_y)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_y. thread_in_y = ',thread_in_y,', ny_sub-1 = ',ny_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        block_in_z = (nz_sub-1)/thread_in_z
        if(block_in_z.eq.0 .or. mod((nz_sub-1), thread_in_z)) then
            print '(a,i5,a,i5)', '[Error] nz_sub-1 should be a multiple of thread_in_z. thread_in_z = ',thread_in_z,', nz_sub-1 = ',nz_sub-1
            call MPI_Finalize(ierr)
            stop
        endif

        threads = dim3(thread_in_x, thread_in_y, thread_in_z)
        blocks  = dim3(block_in_x, block_in_y, block_in_z)
    
        ! Simulation begins
        t_curr = tStart
        dt = dtstart
    
        do time_step = 1, Tmax

            t_curr = t_curr + dt
            if(myrank==0) write(*,*) '[Main] Current time step = ', time_step
        
            theta_d = theta

            call build_RHS_cuda<<<blocks, threads>>>(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, Ct_half_over_dy, Ct_half_over_dz)

            ! solve in the z-direction.
            allocate( ap_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), am_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), &
                    ac_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), ad_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1) )

            ! Create a PaScaL_TDMA plan for the tridiagonal systems.
            call PaScaL_TDMA_plan_many_create_cuda(pz_many, nx_sub-1, nz_sub-1, ny_sub-1, comm_1d_z%myrank, comm_1d_z%nprocs, comm_1d_z%mpi_comm)

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSz_cuda<<<blocks, threads>>>(ap_d, am_d, ac_d, ad_d, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dz, dt)

            !Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(pz_many, am_d, ac_d, ap_d, ad_d, nx_sub-1, nz_sub-1, ny_sub-1)

            ! Return the solution to the r.h.s. plane-by-plane
            call transpose_kij2ijk<<<blocks, threads>>>(ad_d, rhs_d, nx_sub, ny_sub, nz_sub)

            ! Destroy the PaScaL_TDMA plan for the tridiagonal systems.
            call PaScaL_TDMA_plan_many_destroy_cuda(pz_many,comm_1d_z%nprocs)
            deallocate( ap_d, am_d, ac_d, ad_d )

            ! solve in the y-direction.
            allocate( ap_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), am_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), &
                    ac_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), ad_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1) )

            ! Create a PaScaL_TDMA plan for the tridiagonal systems.
            call PaScaL_TDMA_plan_many_create_cuda(py_many, nx_sub-1, ny_sub-1, nz_sub-1, comm_1d_y%myrank, comm_1d_y%nprocs, comm_1d_y%mpi_comm)

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSy_cuda<<<blocks, threads>>>(ap_d, am_d, ac_d, ad_d, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dy, dt)

            ! Solve the tridiagonal systems under the defined plan.
            call PaScaL_TDMA_many_solve_cuda(py_many, am_d, ac_d, ap_d, ad_d, nx_sub-1, ny_sub-1, nz_sub-1)

            ! Return the solution to the r.h.s. plane-by-plane.
            call transpose_jik2ijk<<<blocks, threads>>>(ad_d, rhs_d, nx_sub, ny_sub, nz_sub)

            call PaScaL_TDMA_plan_many_destroy_cuda(py_many,comm_1d_y%nprocs)
            deallocate( ap_d, am_d, ac_d, ad_d )

            ! solve in the x-direction.
            allocate( ap_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), am_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), &
                    ac_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), ad_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) )

            ! Create a PaScaL_TDMA plan for the tridiagonal systems.
            call PaScaL_TDMA_plan_many_create_cuda(px_many, ny_sub-1, nx_sub-1, nz_sub-1, comm_1d_x%myrank, comm_1d_x%nprocs, comm_1d_x%mpi_comm)

            ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
            call build_LHSx_cuda<<<blocks, threads>>>(ap_d, am_d, ac_d, ad_d, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, Ct_half_over_dx, dt)

            ! Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
            call PaScaL_TDMA_many_solve_cycle_cuda(px_many, am_d, ac_d, ap_d, ad_d, ny_sub-1, nx_sub-1, nz_sub-1)

            ! Return the solution to theta plane-by-plane.
            call update_theta_cuda<<<blocks, threads>>>(ad_d, theta_d, nx_sub, ny_sub, nz_sub)

            call PaScaL_TDMA_plan_many_destroy_cuda(px_many,comm_1d_x%nprocs)
            deallocate( ap_d, am_d, ac_d, ad_d )

            theta = theta_d
            ! Update ghostcells from the solution.
            call mpi_subdomain_ghostcell_update(theta, comm_1d_x, comm_1d_y, comm_1d_z)

        end do
        deallocate(rhs_d, theta_d)

    end subroutine solve_theta_plan_many_cuda

    !>
    !> @brief   Modified Thomas algorithm : Update solution
    !> @param   plan        Plan for many tridiagonal systems of equations
    !> @param   A           Coefficients in lower diagonal elements
    !> @param   C           Coefficients in upper diagonal elements
    !> @param   D           Coefficients in right-hand side terms
    !> @param   n_sys       Number of tridiagonal systems per process
    !> @param   n_row       Number of rows in each process, size of a tridiagonal matrix N divided by nprocs
    !>
    attributes(global) subroutine build_RHS_cuda(theta_d, rhs_d, dmx_sub_d, dmy_sub_d, dmz_sub_d, jpbc_index_d, jmbc_index_d, thetaBC3_sub_d, thetaBC4_sub_d, nx_sub, ny_sub, nz_sub, coefx, coefy, coefz)

        implicit none

        double precision, device, intent(in)    :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)          ! r.h.s. array
        double precision, device, intent(inout) :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device :: dmz_sub_d(0:nz_sub), dmy_sub_d(0:ny_sub), dmx_sub_d(0:nx_sub)
        double precision, device :: thetaBC3_sub_d(0:nx_sub, 0:nz_sub), thetaBC4_sub_d(0:nx_sub, 0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, coefy, coefz
        integer, device :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)
        double precision, shared        :: theta_block(*)

        ! Loop and index variables
        integer :: i,j,k
        integer :: ip, jp, kp
        integer :: im, jm, km
        integer :: jem, jep

        ! Temporary shared memory

        ! Temporary variables for coefficient computations
        double precision :: dedx1, dedx2, dedy3, dedy4, dedz5, dedz6    ! Derivative terms
        double precision :: viscous_e1, viscous_e2, viscous_e3, viscous ! Viscous terms
        double precision :: ebc_down, ebc_up, ebc                       ! Boundary terms
        double precision :: eAPI, eAMI, eACI                            ! Diffusion treatment terms in x-direction
        double precision :: eAPJ, eAMJ, eACJ                            ! Diffusion treatment terms in y-direction
        double precision :: eAPK, eAMK, eACK                            ! Diffusion treatment terms in z-direction
        double precision :: eRHS                                        ! From eAPI to eACK

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        ip = i+1
        im = i-1

        jp = j+1
        jm = j-1

        kp = k+1
        km = k-1

        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)

        ! Diffusion term
        dedx1 = (  theta_d(i ,j ,k ) - theta_d(im,j ,k )  )/dmx_sub_d(i )
        dedx2 = (  theta_d(ip,j ,k ) - theta_d(i ,j ,k )  )/dmx_sub_d(ip)  
        dedy3 = (  theta_d(i ,j ,k ) - theta_d(i ,jm,k )  )/dmy_sub_d(j )
        dedy4 = (  theta_d(i ,jp,k ) - theta_d(i ,j ,k )  )/dmy_sub_d(jp)
        dedz5 = (  theta_d(i ,j ,k ) - theta_d(i ,j ,km)  )/dmz_sub_d(k )
        dedz6 = (  theta_d(i ,j ,kp) - theta_d(i ,j ,k )  )/dmz_sub_d(kp)

        viscous_e1 = coefx*(dedx2 - dedx1)
        viscous_e2 = coefy*(dedy4 - dedy3)
        viscous_e3 = coefz*(dedz6 - dedz5)
        viscous = (viscous_e1 + viscous_e2 + viscous_e3)
        
        ! Boundary treatment for the y-direction only
        ebc_down = coefy/dmy_sub_d(j)*thetaBC3_sub_d(i,k)
        ebc_up = coefy/dmy_sub_d(jp)*thetaBC4_sub_d(i,k)
        ebc = dble(1.0d0 - jem)*ebc_down + dble(1. - jep)*ebc_up

        ! Diffusion term from incremental notation in next time step: x-direction
        eAPI = -coefx/dmx_sub_d(ip)
        eAMI = -coefx/dmx_sub_d(i )
        eACI =  coefx*( 1.d0/dmx_sub_d(ip) + 1.d0/dmx_sub_d(i) )

        ! Diffusion term from incremental notation in next time step: z-direction
        eAPK = -coefz/dmz_sub_d(kp)
        eAMK = -coefz/dmz_sub_d(k )
        eACK =  coefz*( 1.d0/dmz_sub_d(kp) + 1.d0/dmz_sub_d(k) )

        ! Diffusion term from incremental notation in next time step: y-direction
        eAPJ = -coefy*( 1.d0/dmy_sub_d(jp) )*dble(jep)
        eAMJ = -coefy*( 1.d0/dmy_sub_d(j ) )*dble(jem)
        eACJ =  coefy*( 1.d0/dmy_sub_d(jp) + 1.d0/dmy_sub_d(j) )
        
        eRHS = eAPK*theta_d(i,j,kp) + eACK*theta_d(i,j,k) + eAMK*theta_d(i,j,km)      &
            & + eAPJ*theta_d(i,jp,k) + eACJ*theta_d(i,j,k) + eAMJ*theta_d(i,jm,k)      &
            & + eAPI*theta_d(ip,j,k) + eACI*theta_d(i,j,k) + eAMI*theta_d(im,j,k)

        ! r.h.s. term 
        rhs_d(i,j,k) = viscous + ebc - eRHS

    end subroutine build_RHS_cuda

    attributes(global) subroutine build_LHSz_cuda(ap_d, am_d, ac_d, ad_d, rhs_d, dmz_sub_d, nx_sub, ny_sub, nz_sub, coefz, dt)

        implicit none

        double precision, device, intent(inout) :: ap_d(nz_sub-1, nx_sub-1, ny_sub-1), am_d(nz_sub-1, nx_sub-1, ny_sub-1)
        double precision, device, intent(inout) :: ac_d(nz_sub-1, nx_sub-1, ny_sub-1), ad_d(nz_sub-1, nx_sub-1, ny_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmz_sub_d(0:nz_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefz, dt
        integer :: i,j,k
        integer :: kp

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        kp = k+1

        ap_d(k,i,j) = -coefz/dmz_sub_d(kp)*dt
        am_d(k,i,j) = -coefz/dmz_sub_d(k )*dt
        ac_d(k,i,j) =  coefz*( 1.d0/dmz_sub_d(kp) + 1.d0/dmz_sub_d(k) )*dt + 1.d0
        ad_d(k,i,j) = rhs_d(i,j,k)*dt

    end subroutine build_LHSz_cuda

    attributes(global) subroutine build_LHSy_cuda(ap_d, am_d, ac_d, ad_d, rhs_d, dmy_sub_d, jpbc_index_d, jmbc_index_d, nx_sub, ny_sub, nz_sub, coefy, dt)

        implicit none

        double precision, device, intent(inout) :: ap_d(ny_sub-1, nx_sub-1, nz_sub-1), am_d(ny_sub-1, nx_sub-1, nz_sub-1)
        double precision, device, intent(inout) :: ac_d(ny_sub-1, nx_sub-1, nz_sub-1), ad_d(ny_sub-1, nx_sub-1, nz_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmy_sub_d(0:ny_sub)
        integer, device, intent(in)             :: jpbc_index_d(0:ny_sub), jmbc_index_d(0:ny_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefy, dt
        integer :: i,j,k
        integer :: jp, jm, jep, jem

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        jp = j + 1
        jm = j - 1
        jep = jpbc_index_d(j)
        jem = jmbc_index_d(j)
        
        ap_d(j,i,k) = -coefy/dmy_sub_d(jp)*dble(jep)*dt
        am_d(j,i,k) = -coefy/dmy_sub_d(j )*dble(jem)*dt
        ac_d(j,i,k) =  coefy*( 1.d0/dmy_sub_d(jp) + 1.d0/dmy_sub_d(j) )*dt + 1.d0
        ad_d(j,i,k) = rhs_d(i,j,k)

    end subroutine build_LHSy_cuda

    attributes(global) subroutine build_LHSx_cuda(ap_d, am_d, ac_d, ad_d, rhs_d, dmx_sub_d, nx_sub, ny_sub, nz_sub, coefx, dt)

        implicit none

        double precision, device, intent(inout) :: ap_d(nx_sub-1, ny_sub-1, nz_sub-1), am_d(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(inout) :: ac_d(nx_sub-1, ny_sub-1, nz_sub-1), ad_d(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(in)    :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        double precision, device, intent(in)    :: dmx_sub_d(0:nx_sub)
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        double precision, value, intent(in)     :: coefx, dt
        integer :: i,j,k
        integer :: ip,im

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        ip = i+1
        im = i-1

        ap_d(i,j,k) = -coefx/dmx_sub_d(ip)*dt
        am_d(i,j,k) = -coefx/dmx_sub_d(i )*dt
        ac_d(i,j,k) =  coefx*( 1.d0/dmx_sub_d(ip) + 1.d0/dmx_sub_d(i) )*dt + 1.d0
        ad_d(i,j,k) = rhs_d(i,j,k)

    end subroutine build_LHSx_cuda


    attributes(global) subroutine transpose_kij2ijk(ad_d, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_d(nz_sub-1, nx_sub-1, ny_sub-1)
        double precision, device, intent(out)   :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_d(k,i,j)

    end subroutine transpose_kij2ijk

    attributes(global) subroutine transpose_jik2ijk(ad_d, rhs_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_d(ny_sub-1, nx_sub-1, nz_sub-1)
        double precision, device, intent(out)   :: rhs_d(nx_sub-1, ny_sub-1, nz_sub-1)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        rhs_d(i,j,k)=ad_d(j,i,k)

    end subroutine transpose_jik2ijk

    attributes(global) subroutine update_theta_cuda(ad_d, theta_d, nx_sub, ny_sub, nz_sub)

        implicit none

        double precision, device, intent(in)    :: ad_d(nx_sub-1, ny_sub-1, nz_sub-1)
        double precision, device, intent(inout) :: theta_d(0:nx_sub, 0:ny_sub, 0:nz_sub)            ! r.h.s. array
        integer, value, intent(in)              :: nx_sub, ny_sub, nz_sub
        integer :: i,j,k

        i = (blockidx%x-1) * blockdim%x + threadidx%x
        j = (blockidx%y-1) * blockdim%y + threadidx%y
        k = (blockidx%z-1) * blockdim%z + threadidx%z

        theta_d(i,j,k) = theta_d(i,j,k) + ad_d(i,j,k)

    end subroutine update_theta_cuda

end module solve_theta_cuda


