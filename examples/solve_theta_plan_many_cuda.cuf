!======================================================================================================================
!> @file        solve_theta_plan_many.f90
!> @brief       This file contains a solver subroutine for the example problem of PaScaL_TDMA.
!> @details     The target example problem is the three-dimensional time-dependent heat conduction problem 
!>              in a unit cube domain applied with the boundary conditions of vertically constant temperature 
!>              and horizontally periodic boundaries.
!> @author      
!>              - Kiha Kim (k-kiha@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>              - Ji-Hoon Kang (jhkang@kisti.re.kr), Korea Institute of Science and Technology Information
!>              - Jung-Il Choi (jic@yonsei.ac.kr), Department of Computational Science & Engineering, Yonsei University
!>
!> @date        June 2019
!> @version     1.0
!> @par         Copyright
!>              Copyright (c) 2019 Kiha Kim and Jung-Il choi, Yonsei University and 
!>              Ji-Hoon Kang, Korea Institute of Science and Technology Information, All rights reserved.
!> @par         License     
!>              This project is release under the terms of the MIT License (see LICENSE in )
!======================================================================================================================

!>
!> @brief       An example solver for many tridiagonal systems of equations using PaScaL_TDMA.
!> @details     This subroutine is for many tridiagonal systems of equations.
!>              It solves the the three-dimensional time-dependent heat conduction problem using PaScaL_TDMA.
!>              PaScaL_TDMA plans are created for many tridiagonal systems of equations and
!>              the many tridiagonal systems are solved plane-by-plane.
!> @param       theta       Main 3-D variable to be solved
!>
subroutine solve_theta_plan_many_cuda(theta)

    use omp_lib
    use mpi
    use global
    use mpi_subdomain
    use mpi_topology
    use PaScaL_TDMA_cuda
    implicit none
    double precision, dimension(0:nx_sub, 0:ny_sub, 0:nz_sub), intent(inout) :: theta
    
    ! Loop and index variables
    integer :: i,j,k
    integer :: ip, jp, kp
    integer :: im, jm, km
    integer :: jem, jep

    integer :: n_thds
    integer :: ti
    integer :: icur, jcur, kcur, n_team, n_remain
    integer :: ierr

    ! Temporary variables for coefficient computations
    double precision :: dedx1, dedx2, dedy3, dedy4, dedz5, dedz6    ! Derivative terms
    double precision :: viscous_e1, viscous_e2, viscous_e3, viscous ! Viscous terms
    double precision :: ebc_down, ebc_up, ebc                       ! Boundary terms
    double precision :: eAPI, eAMI, eACI                            ! Diffusion treatment terms in x-direction
    double precision :: eAPJ, eAMJ, eACJ                            ! Diffusion treatment terms in y-direction
    double precision :: eAPK, eAMK, eACK                            ! Diffusion treatment terms in z-direction
    double precision :: eRHS                                        ! From eAPI to eACK

    double precision, allocatable, dimension(:, :, :) :: rhs            ! r.h.s. array
    double precision, allocatable, dimension(:, :, :) :: apt, amt, act, adt    ! Coefficient of tridiagonal matrix
    double precision, allocatable, device, dimension(:,:,:) :: ap_d, am_d, ac_d, ad_d ! Coefficient of tridiagonal matrix

    type(ptdma_plan_many_cuda) :: px_many, pz_many , py_many  ! Plan for many tridiagonal systems of equations

    n_thds = omp_get_max_threads()
    if(mod(nx_sub-1, n_thds).ne.0) then
        print '(a,i4,a,i4)', '[Error] nxm per core should be a multiple of n_thds: nxm = ',nx_sub-1,', n_thds = ',n_thds
        call MPI_Finalize(ierr)
        stop
    endif

    if(mod(ny_sub-1, n_thds).ne.0) then
        print '(a,i4,a,i4)', '[Error] nym per core should be a multiple of n_thds: nym = ',ny_sub-1,', n_thds = ',n_thds
        call MPI_Finalize(ierr)
        stop
    endif

    if(mod(nz_sub-1, n_thds).ne.0) then
        print '(a,i4,a,i4)', '[Error] nzm per core should be a multiple of n_thds: nzm = ',nz_sub-1,', n_thds = ',n_thds
        call MPI_Finalize(ierr)
        stop
    endif

    ! Calculating r.h.s.
    allocate( rhs(0:nx_sub, 0:ny_sub, 0:nz_sub))

    do k = 1, nz_sub-1
        kp = k+1
        km = k-1

        do j = 1, ny_sub-1
            jp = j + 1
            jm = j - 1
            jep = jpbc_index(j)
            jem = jmbc_index(j)

            do i = 1, nx_sub-1
                ip = i+1
                im = i-1

                ! Diffusion term
                dedx1 = (  theta(i ,j ,k ) - theta(im,j ,k )  )/dmx_sub(i )
                dedx2 = (  theta(ip,j ,k ) - theta(i ,j ,k )  )/dmx_sub(ip)  
                dedy3 = (  theta(i ,j ,k ) - theta(i ,jm,k )  ) /dmy_sub(j )
                dedy4 = (  theta(i ,jp,k ) - theta(i ,j ,k )  ) /dmy_sub(jp)
                dedz5 = (  theta(i ,j ,k ) - theta(i ,j ,km)  )/dmz_sub(k )
                dedz6 = (  theta(i ,j ,kp) - theta(i ,j ,k )  )/dmz_sub(kp)

                viscous_e1 = 1.d0/dx*(dedx2 - dedx1)
                viscous_e2 = 1.d0/dy*(dedy4 - dedy3)
                viscous_e3 = 1.d0/dz*(dedz6 - dedz5)
                viscous = 0.5d0*Ct*(viscous_e1 + viscous_e2 + viscous_e3) 
                
                ! Boundary treatment for the y-direction only
                ebc_down = 0.5d0*Ct/dy/dmy_sub(j)*thetaBC3_sub(i,k)
                ebc_up = 0.5d0*Ct/dy/dmy_sub(jp)*thetaBC4_sub(i,k)
                ebc = dble(1. - jem)*ebc_down + dble(1. - jep)*ebc_up

                ! Diffusion term from incremental notation in next time step: x-direction
                eAPI = -0.5d0*Ct/dx/dmx_sub(ip)
                eAMI = -0.5d0*Ct/dx/dmx_sub(i )
                eACI =  0.5d0*Ct/dx*( 1.d0/dmx_sub(ip) + 1.d0/dmx_sub(i) )

                ! Diffusion term from incremental notation in next time step: z-direction
                eAPK = -0.5d0*Ct/dz/dmz_sub(kp)
                eAMK = -0.5d0*Ct/dz/dmz_sub(k )
                eACK =  0.5d0*Ct/dz*( 1.d0/dmz_sub(kp) + 1.d0/dmz_sub(k) )

                ! Diffusion term from incremental notation in next time step: y-direction
                eAPJ = -0.5d0*Ct/dy*( 1.d0/dmy_sub(jp) )*dble(jep)
                eAMJ = -0.5d0*Ct/dy*( 1.d0/dmy_sub(j ) )*dble(jem)
                eACJ =  0.5d0*Ct/dy*( 1.d0/dmy_sub(jp) + 1.d0/dmy_sub(j) )
                
                eRHS = eAPK*theta(i,j,kp) + eACK*theta(i,j,k) + eAMK*theta(i,j,km)      &
                    & + eAPJ*theta(i,jp,k) + eACJ*theta(i,j,k) + eAMJ*theta(i,jm,k)      &
                    & + eAPI*theta(ip,j,k) + eACI*theta(i,j,k) + eAMI*theta(im,j,k)

                ! r.h.s. term 
                rhs(i,j,k) = theta(i,j,k)/dt + viscous + ebc      &
                          & - (theta(i,j,k)/dt + eRHS)
            enddo
        enddo
        ! print '(a10,x,4(i5,x))', '[RHS-Team]', k, omp_get_thread_num(), omp_get_num_threads(), omp_get_max_threads()
    enddo

    ! solve in the z-direction.
    allocate( apt(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), amt(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), &
              act(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), adt(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1) )

    allocate( ap_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), am_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), &
              ac_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1), ad_d(1:nz_sub-1, 1:nx_sub-1, 1:ny_sub-1) )

    ! Create a PaScaL_TDMA plan for the tridiagonal systems.
    call PaScaL_TDMA_plan_many_create_cuda(pz_many, nx_sub-1, nz_sub-1, ny_sub-1, comm_1d_z%myrank, comm_1d_z%nprocs, comm_1d_z%mpi_comm)

    ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
    do j = 1, ny_sub-1
        do k = 1, nz_sub-1
            kp = k+1
            do i = 1, nx_sub-1
                apt(k,i,j) = -0.5d0*Ct/dz/dmz_sub(kp)*dt
                amt(k,i,j) = -0.5d0*Ct/dz/dmz_sub(k )*dt
                act(k,i,j) =  0.5d0*Ct/dz*( 1.d0/dmz_sub(kp) + 1.d0/dmz_sub(k) )*dt + 1.d0
                adt(k,i,j) = rhs(i,j,k)*dt
            enddo
        enddo
    enddo

    !Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
    ap_d = apt
    ac_d = act
    am_d = amt
    ad_d = adt
    call PaScaL_TDMA_many_solve_cycle_cuda(pz_many, am_d, ac_d, ap_d, ad_d, nx_sub-1, nz_sub-1, ny_sub-1)
    adt = ad_d

    ! Return the solution to the r.h.s. plane-by-plane
    do j = 1, ny_sub-1
        do k = 1, nz_sub-1
            rhs(1:nx_sub-1,j,k)=adt(k,1:nx_sub-1,j)
        enddo
    enddo

    ! Destroy the PaScaL_TDMA plan for the tridiagonal systems.
    call PaScaL_TDMA_plan_many_destroy_cuda(pz_many,comm_1d_z%nprocs)
    deallocate( apt, amt, act, adt )
    deallocate( ap_d, am_d, ac_d, ad_d )

    ! solve in the y-direction.
    allocate( apt(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), amt(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), &
              act(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), adt(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1) )

    allocate( ap_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), am_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), &
              ac_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1), ad_d(1:ny_sub-1, 1:nx_sub-1, 1:nz_sub-1) )

    ! Create a PaScaL_TDMA plan for the tridiagonal systems.
    call PaScaL_TDMA_plan_many_create_cuda(py_many, nx_sub-1, ny_sub-1, nz_sub-1, comm_1d_y%myrank, comm_1d_y%nprocs, comm_1d_y%mpi_comm)

    ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
    do k = 1, nz_sub-1
        do j = 1, ny_sub-1
            jp = j + 1
            jm = j - 1
            jep = jpbc_index(j)
            jem = jmbc_index(j)
            
            do i = 1, nx_sub-1
                apt(j,i,k) = -0.5d0*Ct/dy/dmy_sub(jp)*dble(jep)*dt
                amt(j,i,k) = -0.5d0*Ct/dy/dmy_sub(j )*dble(jem)*dt
                act(j,i,k) =  0.5d0*Ct/dy*( 1.d0/dmy_sub(jp) + 1.d0/dmy_sub(j) )*dt + 1.d0
                adt(j,i,k) = rhs(i,j,k)
            end do
        end do
    end do

    ! Solve the tridiagonal systems under the defined plan.
    ap_d = apt
    ac_d = act
    am_d = amt
    ad_d = adt
    call PaScaL_TDMA_many_solve_cuda(py_many, am_d, ac_d, ap_d, ad_d, nx_sub-1, ny_sub-1, nz_sub-1)
    adt = ad_d

    ! Return the solution to the r.h.s. plane-by-plane.
    do k = 1, nz_sub-1
        do j = 1, ny_sub-1
            do i = 1, nx_sub-1
                rhs(i,j,k)=adt(j,i,k)
            enddo
        enddo
    end do

    call PaScaL_TDMA_plan_many_destroy_cuda(py_many,comm_1d_y%nprocs)
    deallocate( apt, amt, act, adt )
    deallocate( ap_d, am_d, ac_d, ad_d )

    ! solve in the x-direction.
    allocate( apt(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), amt(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), &
              act(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), adt(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) )

    allocate( ap_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), am_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), &
              ac_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1), ad_d(1:nx_sub-1, 1:ny_sub-1, 1:nz_sub-1) )

    ! Create a PaScaL_TDMA plan for the tridiagonal systems.
    call PaScaL_TDMA_plan_many_create_cuda(px_many, ny_sub-1, nx_sub-1, nz_sub-1, comm_1d_x%myrank, comm_1d_x%nprocs, comm_1d_x%mpi_comm)

    ! Build a coefficient matrix for the tridiagonal systems into a 2D array.
    do k = 1, nz_sub-1
        do j = 1, ny_sub-1
            do i = 1, nx_sub-1
                ip = i+1
                im = i-1

                apt(i,j,k) = -0.5d0*Ct/dx/dmx_sub(ip)*dt
                amt(i,j,k) = -0.5d0*Ct/dx/dmx_sub(i )*dt
                act(i,j,k) =  0.5d0*Ct/dx*( 1.d0/dmx_sub(ip) + 1.d0/dmx_sub(i) )*dt + 1.d0
                adt(i,j,k) = rhs(i,j,k)
            enddo
        enddo
    enddo
    ! Solve the tridiagonal systems under the defined plan with periodic boundary conditions.
    ap_d = apt
    ac_d = act
    am_d = amt
    ad_d = adt
    call PaScaL_TDMA_many_solve_cycle_cuda(px_many, am_d, ac_d, ap_d, ad_d, ny_sub-1, nx_sub-1, nz_sub-1)
    adt = ad_d

    ! Return the solution to theta plane-by-plane.
    do k = 1, nz_sub-1
        do j = 1, ny_sub-1
            theta(1:nx_sub-1,j,k) = theta(1:nx_sub-1,j,k) + adt(1:nx_sub-1,j,k)
        enddo
    enddo

    call PaScaL_TDMA_plan_many_destroy_cuda(px_many,comm_1d_x%nprocs)

    deallocate( apt, amt, act, adt )
    deallocate( ap_d, am_d, ac_d, ad_d )

    deallocate(rhs)

    ! Update ghostcells from the solution.
    call mpi_subdomain_ghostcell_update(theta, comm_1d_x, comm_1d_y, comm_1d_z)

end subroutine solve_theta_plan_many_cuda

